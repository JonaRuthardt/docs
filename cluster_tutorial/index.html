<!DOCTYPE html>

<html data-bs-theme="light" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Jona Ruthardt" name="author"/>
<link href="https://JonaRuthardt.github.io/docs/cluster_tutorial/" rel="canonical"/>
<link href="../img/favicon.ico" rel="shortcut icon"/>
<title>Using a Compute Cluster - Docs</title>
<link href="../css/bootstrap.min.css" rel="stylesheet"/>
<link href="../css/fontawesome.min.css" rel="stylesheet"/>
<link href="../css/brands.min.css" rel="stylesheet"/>
<link href="../css/solid.min.css" rel="stylesheet"/>
<link href="../css/v4-font-face.min.css" rel="stylesheet"/>
<link href="../css/base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/color-brewer.min.css" id="hljs-light" rel="stylesheet"/>
<link disabled="" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" id="hljs-dark" rel="stylesheet"/>
<link href="../custom.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</head>
<body>
<div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
<div class="container">
<a class="navbar-brand" href="..">Docs</a>
<!-- Expander button -->
<button aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-bs-target="#navbar-collapse" data-bs-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<!-- Expanded navigation -->
<div class="navbar-collapse collapse" id="navbar-collapse">
<!-- Main navigation -->
<ul class="nav navbar-nav">
<li class="nav-item dropdown">
<a aria-current="page" aria-expanded="false" class="nav-link dropdown-toggle active" data-bs-toggle="dropdown" href="#" role="button">Tutorials</a>
<ul class="dropdown-menu">
<li>
<a aria-current="page" class="dropdown-item active" href="./">Using a Compute Cluster</a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<a aria-expanded="false" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" href="#" role="button">Documentation</a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href=".."></a>
</li>
</ul>
</li>
<li class="nav-item dropdown">
<a aria-expanded="false" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" href="#" role="button">Resources</a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href=".."></a>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav ms-md-auto">
<li class="nav-item">
<a class="nav-link" data-bs-target="#mkdocs_search_modal" data-bs-toggle="modal" href="#">
<i class="fa fa-search"></i> Search
                            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="container">
<div class="row">
<div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
<div class="navbar-header">
<button class="navbar-toggler collapsed" data-bs-target="#toc-collapse" data-bs-toggle="collapse" title="Table of Contents" type="button">
<span class="fa fa-angle-down"></span>
</button>
</div>
<div class="navbar-collapse collapse card bg-body-tertiary" id="toc-collapse">
<ul class="nav flex-column">
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#working-with-clusters-for-deep-learning">Working with Clusters for Deep Learning</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#the-alex-cluster">The Alex Cluster</a>
<ul class="nav flex-column">
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#what-is-a-cluster">What is a Cluster?</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#compute-architecture">Compute Architecture</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#filesystems-and-data-storage">Filesystems and Data Storage</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#interaction-between-components">Interaction Between Components</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#connecting-to-the-cluster">Connecting to the Cluster</a>
<ul class="nav flex-column">
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#connecting-to-alex-via-ssh">Connecting to Alex via SSH</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#transferring-files-rsyncscp">Transferring Files (rsync/scp)</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#local-vs-remote-development">Local vs. Remote Development</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#setting-up-the-environment">Setting up the Environment</a>
<ul class="nav flex-column">
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#modules">Modules</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#conda">Conda</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#scheduling-jobs">Scheduling Jobs</a>
<ul class="nav flex-column">
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#slurm-basics">SLURM Basics</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#job-files">Job Files</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#job-arrays">Job Arrays</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#interactive-sessions">Interactive Sessions</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="2"><a class="nav-link" href="#attaching-to-running-jobs">Attaching to Running Jobs</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#best-practices_1">Best Practices</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#troubleshooting-and-faqs">Troubleshooting and FAQs</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-bs-level="1"><a class="nav-link" href="#acknowledgements">Acknowledgements</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</div>
</div></div>
<div class="col-md-9" role="main">
<h1 id="working-with-clusters-for-deep-learning">Working with Clusters for Deep Learning</h1>
<p><strong>Created by <a href="https://jonaruthardt.github.io/">Jona Ruthardt</a></strong> (01/2025)</p>
<hr/>
<!-- _hide: true -->
<div class="admonition note">
<p class="admonition-title">Before you start ‚Ä¶</p>
<p>This tutorial will help you start using SLURM-based GPU clusters for your deep learning projects. Its goal is to guide you through the process of accessing and effectively utilizing a cluster, particularly for people who may be new to cluster computing. </p>
<p>By the end of the tutorial, you will understand:</p>
<ol>
<li><strong>What a GPU cluster is</strong> and how its components interact</li>
<li><strong>How to connect to a cluster</strong> and set up your environment</li>
<li><strong>How to schedule and manage jobs</strong> to access GPUs for your experiment</li>
<li><strong>Best practices for working with a cluster</strong> to maximize your productivity and the efficiency of your experiments</li>
</ol>
<p>While this tutorial was created for courses at <a href="https://www.utn.de">UTN</a> and, therefore, provides some instructions specific to the <a href="https://doc.nhr.fau.de/clusters/alex/">Alex cluster by FAU</a>, most aspects translate to other compute and GPU clusters as well. It can still serve as a helpful resource and introduction to the topic.</p>
<p>You will find additional resources and practical examples that help you get started <a href="https://github.com/JonaRuthardt/HowToCluster">in this github repository</a>. </p>
</div>
<hr/>
<h1 id="the-alex-cluster">The Alex Cluster</h1>
<p>To work effectively with a compute cluster, it is essential to be familiar with its high-level architecture and which components it is comprised of. The following sections provide a basic introduction to the Alex cluster by FAU. For more detailed information, see the <a href="https://doc.nhr.fau.de/clusters/alex/">official documentation</a>. </p>
<h2 id="what-is-a-cluster">What is a Cluster?</h2>
<p>A (GPU) cluster is a collection of interconnected computers (<strong><em>nodes</em></strong>) that are equipped with Graphics Processing Units (<strong><em>GPUs</em></strong>). These GPUs are specialized hardware accelerators designed for highly parallel computations, making them particularly effective for tasks like deep learning, machine learning, and other computationally intensive applications.</p>
<hr/>
<h3 id="why-use-gpu-clusters">Why Use GPU Clusters?</h3>
<p>When scaling up model architectures and the volume of training/evaluation data, computational demands can quickly overwhelm personal computers or local GPUs. In contrast, GPU clusters offer the following advantages: </p>
<ul>
<li><strong>Scalability</strong> (more available resources ‚Üí train larger models; run many tasks at once)</li>
<li><strong>Efficiency</strong> (higher performance resources ‚Üí train models faster)</li>
<li><strong>Advanced hardware</strong> (access cutting-edge hardware)</li>
</ul>
<p><strong>Additional benefit for students:</strong> familiarize yourself with tools and infrastructure used in the industry and gain practical experience</p>
<hr/>
<h3 id="differences-to-local-computing">Differences to Local Computing</h3>
<table>
<thead>
<tr>
<th></th>
<th>Local Compute</th>
<th>GPU Cluster</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hardware</td>
<td>Limited to resources on your personal device</td>
<td>Access to powerful GPUs</td>
</tr>
<tr>
<td>Performance</td>
<td>Suitable for small-scale tasks</td>
<td>Optimized for large-scale computations</td>
</tr>
<tr>
<td>Accessibility</td>
<td>Direct interaction with your system</td>
<td>Accessed remotely via network connection</td>
</tr>
<tr>
<td>Resource Sharing</td>
<td>Dedicated to one user</td>
<td>Shared across multiple users with job scheduling</td>
</tr>
<tr>
<td>Scalability</td>
<td>Restricted to the one device</td>
<td>Utilize multiple GPUs (and nodes)</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="compute-architecture">Compute Architecture</h2>
<p>On a high level, a cluster can be conceptualized as individual computers (nodes) connected to a central entity and can exchange information. Nonetheless, each node has its own CPU, memory, and storage. A cluster is typically made up of at least two types of nodes:</p>
<h3 id="login-nodes">Login Nodes</h3>
<p>The login nodes serve as your way to <strong>access the cluster</strong>. You are automatically connected to one of these nodes when you connect to the cluster. They provide you with a basic interface to interact with the cluster (e.g., file management, coding, submitting jobs). However, they are not intended for serious computational workloads or running jobs. Since they are shared across multiple users, be mindful and <strong>avoid running resource-intensive processes</strong> on login nodes. </p>
<h3 id="compute-nodes">Compute Nodes</h3>
<p>The compute/GPU nodes are where <strong>computationally demanding tasks</strong> are executed. Connecting directly to a compute node is usually not possible and it is first necessary to reserve the resources by scheduling a <strong><em>job</em></strong> from a login node.</p>
<p>The scheduler (e.g., SLURM) manages and allocates the cluster‚Äôs resources. User requests are queued and prioritized based on factors like scheduled runtime, resource requirements, time in queue, etc. Once sufficient resources become available, the appropriate node(s) are allocated and usable to run user-specified code. </p>
<p>The GPU nodes on Alex you have access to have the following specs:</p>
<ul>
<li>2x AMD EPYC CPU (2x 64 cores @2.0 GHz)</li>
<li>8x Nvidia A40 GPU (40GB VRAM)</li>
<li>512 GB memory</li>
<li>7 TB node-local storage</li>
</ul>
<p>Typically, compute nodes are divided further into smaller compute units. These units represent a subset of the available resources (such as a specific number of CPU cores or GPUs) and are allocated based on user requests. For example, our compute node with 8 GPUs can be partitioned into 8 smaller units (‚Üí 16 CPU cores, 64GB RAM, ~1TB SSD), depending on the task at hand. This partitioning allows multiple users to share the same node without interfering with each other‚Äôs work, optimizing resource utilization. </p>
<hr/>
<div class="mermaid">graph TD
    %% Define User
    U[User] --&gt;|SSH/Command Access| A[Login Node]

    %% Define Login Node and Compute Nodes
    A --&gt;|Job Submission| B1[Compute Node 1]
    A --&gt; B2[Compute Node 2]
    A --&gt; B3[Compute Node 3]

    %% Define GPUs for each compute node
    subgraph .
        B1 --&gt; G1[GPU 1]
        B1 --&gt; G2[GPU 2]
        B1 --&gt; G3[GPU 3]
        B1 --&gt; G4[GPU 4]
    end

    subgraph .
        B2 --&gt; G5[GPU 1]
        B2 --&gt; G6[GPU 2]
        B2 --&gt; G7[GPU 3]
        B2 --&gt; G8[GPU 4]
    end

    subgraph .
        B3 --&gt; G9[GPU 1]
        B3 --&gt; G10[GPU 2]
        B3 --&gt; G11[GPU 3]
        B3 --&gt; G12[GPU 4]
    end
</div>
<hr/>
<h2 id="filesystems-and-data-storage">Filesystems and Data Storage</h2>
<p>In addition to the storage on the node itself, the cluster has a centralized filesystem that all nodes can access. This allows users to store and manage their data and models. This section will guide you through the cluster‚Äôs available filesystems, their intended use, and best practices for managing your data effectively. Understanding these will help you optimize storage, maintain data integrity, and avoid issues such as quota limits or accidental data loss.</p>
<hr/>
<h3 id="overview">Overview</h3>
<p>The cluster provides several filesystems, each with specific purposes, storage capacities, and backup policies. Here‚Äôs a summary of the most essential ones:</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Purpose</th>
<th>Backup and Snapshots</th>
<th>IO Speed</th>
<th>Quota</th>
</tr>
</thead>
<tbody>
<tr>
<td>$HOME</td>
<td>Important and unrecoverable files (e.g., scripts, results)</td>
<td>Yes</td>
<td>Normal</td>
<td>50 GB</td>
</tr>
<tr>
<td>$HPCVAULT</td>
<td>Long-term storage of data</td>
<td>Yes (infrequent)</td>
<td>Slow</td>
<td>500 GB</td>
</tr>
<tr>
<td>$WORK</td>
<td>General-purpose files (e.g., logs, intermediate results)</td>
<td>No</td>
<td>Normal</td>
<td>1000 GB</td>
</tr>
<tr>
<td>$TMPDIR</td>
<td>Job-specific (!) scratch space</td>
<td>No</td>
<td>Very Fast</td>
<td>None</td>
</tr>
</tbody>
</table>
<hr/>
<h3 id="accessing-storage">Accessing Storage</h3>
<p>The above file systems (except for the node-local $TMPDIR) are mounted across all cluster nodes. You can access the storage directories from anywhere using their predefined environment variables. Here are two examples: </p>
<p><strong>Bash</strong></p>
<ul>
<li>Navigate to your $WORK directory: <code>cd $WORK</code></li>
<li>List all files in your $HPCVAULT directory: <code>ls $HPCVAULT</code></li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="language-python">import os

# Get the path to your $HOME directory
home_path = os.environ.get("HOME")
print(f"Home directory: {home_path}")
</code></pre>
<hr/>
<h3 id="best-practices">Best Practices</h3>
<ul>
<li>Use $HOME for important files due to regular backups and snapshots</li>
<li>Store temporary or large files in $WORK, or $TMPDIR (will be deleted after the job finished!).</li>
<li>Stage your data to node-local $TMPDIR when fast read/write operations are essential (<a href="https://doc.nhr.fau.de/data/staging/?h=data+staging#staging-data-in-and-out">cf</a>)</li>
<li>Regularly monitor quotas (run <code>shownicerquota.pl</code>) and clean up unnecessary files</li>
</ul>
<hr/>
<h3 id="recovering-overwrittendeleted-files">Recovering Overwritten/Deleted Files</h3>
<p>The snapshots taken on $HOME and $HPCVAULT in regular intervals allow you to restore files that may have been deleted: </p>
<ol>
<li>
<p>List the available snapshots in the <code>.snapshots</code> directory of the target folder:</p>
<p><code>bash
$ ls -l $HOME/your-folder/.snapshots/</code></p>
</li>
<li>
<p>Copy the desired version back: </p>
<p><code>bash
$ cp '/path/to/.snapshots/@GMT-TIMESTAMP/file' '/path/to/restore/file'</code></p>
</li>
</ol>
<p><a href="https://doc.nhr.fau.de/data/filesystems/">More information about filesystems</a></p>
<hr/>
<h2 id="interaction-between-components">Interaction Between Components</h2>
<p>Now that you know what kind of basic building blocks make up a cluster, let‚Äôs look at a typical workflow to illustrate how these components work together: </p>
<ol>
<li><strong>Connect to the Login Node</strong>
Use <em>ssh</em> to remotely access the login node</li>
<li><strong>Implement and setup your experiments</strong>
Upload and edit datasets, scripts, etc. to the cluster‚Äôs shared storage system</li>
<li><strong>Submit a Job</strong>
Specify the resource requirements and the code to be executed </li>
<li><strong>Scheduler Allocates Ressources</strong>
Compute node is allocated and your code starts executing</li>
<li><strong>Access Results</strong>
Log back into the login node and find your results at the respective location of the filesystem</li>
</ol>
<div class="mermaid">flowchart TD
    A[User] --&gt;|SSH/scp/rsync| B[Login Node]
    B --&gt;|Submit Job| D[Job Scheduler]
    D --&gt;|Allocate Resources| E[Compute Node]
    C[Shared Storage System] 
    B &lt;--&gt;|Edit Datasets, Scripts, etc.| C
    C --&gt;|Fetch Code| E
    E --&gt;|Store Results| C
</div>
<hr/>
<h1 id="connecting-to-the-cluster">Connecting to the Cluster</h1>
<p>Now that you are familiar with compute clusters, let‚Äôs try using one. </p>
<h2 id="connecting-to-alex-via-ssh">Connecting to Alex via SSH</h2>
<p>We will use the Secure Shell Protocol (SSH) to connect our local machine to the remote cluster. For that, make sure you have a Terminal/PowerShell or SSH client like <a href="https://www.putty.org">PuTTY</a> handy. However, before you can connect, there are a couple of setup steps necessary still. </p>
<hr/>
<h3 id="accepting-cluster-invitation">Accepting Cluster Invitation</h3>
<p>By now, you should have received an invitation to the project <em>‚ÄúUTN AI&amp;Robotics MSc Studierende‚Äù</em> on the <a href="https://portal.hpc.fau.de/">HPC-Portal</a>. Follow these steps to accept the invitation:</p>
<ol>
<li>Open the <a href="https://portal.hpc.fau.de/">HPC-Portal</a> and log in via your UTN account</li>
<li>Go to ‚ÄúUser‚Äù &gt; ‚ÄúYour Invitations‚Äù and accept the invitation</li>
</ol>
<hr/>
<h3 id="configuring-ssh-keys">Configuring SSH Keys</h3>
<p>To be able to access the cluster, authentication of your user is necessary. For this, we will be using SSH key pairs. Follow these steps for generating and uploading the keys:</p>
<ol>
<li>Generate an SSH key pair (secret + public key) on your local machine with <code>ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519_nhr_fau</code> 
You will be asked to (optionally) enter a passphrase</li>
<li>Print out the public key using <code>cat ~/.ssh/id_ed25519_nhr_fau.pub</code> and copy the output</li>
<li>Open the <a href="https://portal.hpc.fau.de/">HPC-Portal</a> and navigate to the ‚ÄúUser‚Äù tab in your account settings and click on ‚ÄúAdd new SSH key‚Äù button. 
Enter an alias name of your choice and paste the copied output of the key file. </li>
</ol>
<hr/>
<h3 id="editing-ssh-config">Editing SSH Config</h3>
<p>The Alex cluster is part of the bigger FAU high-performance compute center which uses a central dialog server from which a so-called <em>proxy jump</em> to the GPU cluster is performed. For this to work, we need to add some configurations to the <code>~/.ssh/config</code> file on your machine. This will ensure the cluster address, authentication method, etc. are all setup correctly when you try to connect. </p>
<p>Replace <code>&lt;HPC account&gt;</code> with your user account (see <a href="https://portal.hpc.fau.de/">HPC-Portal</a> under¬†<em>Your accounts¬†&gt;¬†Active accounts &gt; HPC-Account</em>) in the following template and paste it to your <code>~/.ssh/config</code> file:</p>
<pre><code class="language-bash">Host csnhr.nhr.fau.de csnhr
    HostName csnhr.nhr.fau.de
    User &lt;HPC account&gt;
    IdentityFile ~/.ssh/id_ed25519_nhr_fau
    IdentitiesOnly yes
    PasswordAuthentication no
    PreferredAuthentications publickey
    ForwardX11 no
    ForwardX11Trusted no

Host alex.nhr.fau.de alex
    HostName alex.nhr.fau.de
    User &lt;HPC account&gt;
    ProxyJump csnhr.nhr.fau.de
    IdentityFile ~/.ssh/id_ed25519_nhr_fau
    IdentitiesOnly yes
    PasswordAuthentication no
    PreferredAuthentications publickey
    ForwardX11 no
    ForwardX11Trusted no
</code></pre>
<hr/>
<!-- _hide: true -->
<div class="admonition note">
<p class="admonition-title">Notes about connecting via SSH ‚Ä¶</p>
<ol>
<li>The user name you replace in the config file template will differ from the one used to log into the HPC-Portal. Be sure to use the one specified <em>Your accounts¬†&gt;¬†Active accounts</em> in the HPC-Portal.</li>
<li>The file name and location must be <code>~/.ssh/config</code> (no file extension like .txt). You can edit the file using <code>nano</code> or your text editor of choice.</li>
<li>After submitting the SSH key in the HPC-Portal, it might take up to two hours for the key to be distributed to all systems. During that time, you may not be able to log in yet. </li>
</ol>
</div>
<hr/>
<h3 id="testing-your-connection">Testing Your Connection</h3>
<p>If everything worked correctly, you should now be able to connect to the cluster. For that, first connect to the dialog server using the following command:</p>
<pre><code class="language-bash">ssh csnhr.nhr.fau.de
</code></pre>
<p>If this is your first time connecting to this remote address, you will be asked whether you‚Äôd like to continue. Confirm by entering <code>yes</code>. By typing in the <code>exit</code> command, you can close the connection and return to your local shell. </p>
<p>Now, connect to the Alex cluster directly using <code>ssh alex</code>. Again, you might have to confirm your intent with <code>yes</code> the first time you try to connect. </p>
<hr/>
<h2 id="transferring-files-rsyncscp">Transferring Files (rsync/scp)</h2>
<p>In case you want to manually transfer files from your local system to the cluster or vice versa, you can use the <code>scp</code> command:</p>
<pre><code class="language-bash"># Local file -&gt; cluster
scp -r path/to/file &lt;USERNAME&gt;@&lt;CLUSTER&gt;:/path/to/destination
# Remote file -&gt; local machine
scp -r &lt;USERNAME&gt;@&lt;CLUSTER&gt;:/path/to/file path/to/destination
</code></pre>
<p>Be sure to correctly replace the <code>&lt;USERNAME&gt;</code> and <code>&lt;CLUSTER&gt;</code> tags with your username and the cluster address (e.g., <code>alex</code>), respectively. Also, specify the file/folder you want to copy. </p>
<p>For larger transfers, it can be recommended to use <code>rsync</code> :</p>
<pre><code class="language-bash">rsync -avz path/to/file &lt;USERNAME&gt;@&lt;CLUSTER&gt;:/path/to/destination
</code></pre>
<p><strong>Hint:</strong> by adding the <em>exclude</em> flag (e.g., <code>--exclude *.pt</code>), it is possible to ignore certain files or file types in the download. This can be handy when you only want to transfer your code without the large model checkpoints. </p>
<h2 id="local-vs-remote-development">Local vs. Remote Development</h2>
<p>When working with a cluster, you have two main options for developing and testing your code: <strong>local</strong> or <strong>remote development</strong>. Each has its strengths and weaknesses, and your workflow can depend on your project requirements or personal preferences. Here is an overview of both:</p>
<hr/>
<h3 id="local-development-implement-locally-run-on-cluster">Local Development - Implement Locally, Run on Cluster</h3>
<p><strong>Workflow:</strong></p>
<ul>
<li>Implement and test your code locally on your personal computer</li>
<li>Once the code runs successfully locally, commit and push the changes to the cluster (e.g., via git) for running large-scale experiments</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li><strong>Quick prototyping and debugging</strong> (no waiting for resource allocations)</li>
<li><strong>Resource efficiency</strong> (doesn‚Äôt take up GPUs on the cluster while implementing and testing)</li>
<li><strong>Obligatory version control</strong> (regular commits are necessary)</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Limited resources</strong> (infeasible for models requiring more resources to be loaded/run)</li>
<li><strong>Different environment</strong> (local environment may differ from that of cluster (e.g., CPU vs. GPU, CUDA versions, etc.))</li>
</ul>
<hr/>
<h3 id="remote-development-develop-and-run-on-cluster">Remote Development - Develop and Run on Cluster</h3>
<p><strong>Workflow:</strong></p>
<ul>
<li>Connect to the cluster directly and implement everything there using and IDE/editor like VS Code or PyCharm (or command line tools like <code>vim</code> or <code>nano</code>)</li>
<li>Execute and debug all experiments on the cluster itself</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li><strong>Consistent environment</strong> (development and execution happens in same software and hardware environment)</li>
<li><strong>Less management overhead</strong> (no need to maintain and manage two separate code and data instances)</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Connectivity dependency</strong> (requires internet connection at all times)</li>
<li><strong>Resource availability</strong> (you might need to wait for resources to get allocated)</li>
<li><strong>Inefficient use of resources</strong> (testing and debugging might drastically reduce resource utilization)</li>
</ul>
<hr/>
<!-- ### Remote Development with VS Code

1. Ensure you have VS Code installed on your local machine.
2. In VS Code, install the ‚ÄúRemote Development‚Äù extension package.
3. In the command pallete (cmd/strg + P), select ‚ÄúRemote-SSH: Connect to Host ‚Ä¶‚Äù.
4. If the configuration you created previously is correct, the Alex cluster should already show up as an selectable option.
5. Now, you can use the familiar VS Code interface but interact with files and the command line on the cluster instead of on your machine.

!!! note "About debugging ..."
    If you want to debug and execute code using your VS Code environment, be sure to connect to an interactive node instead of running it on the login node. 

For more information about remote development using SSH, [see the official documentation](https://code.visualstudio.com/docs/remote/ssh).

--- -->
<h1 id="setting-up-the-environment">Setting up the Environment</h1>
<p>Once you are authenticated and logged into the cluster, the next step is to set up your environment further to ensure you can use the necessary software and GPU resources.</p>
<hr/>
<h2 id="modules">Modules</h2>
<p><a href="https://modules.sourceforge.net">Environment modules</a> provide different versions of compilers, applications, and environments and are used by many clusters to dynamically manage software environments. They allow you to load and unload software packages, which ensures compatibility and avoids conflicts between software versions while being easy to use.</p>
<p>Here are the basic commands to work with the <code>modules</code> system:</p>
<ul>
<li><code>module avail</code>: shows all software modules available on the cluster</li>
<li><code>module load/unload &lt;module_name&gt;</code>: load or unload the module you want to use (e.g., <code>python</code>)</li>
<li><code>module list</code>: shows all software modules currently loaded in the environment</li>
<li><code>module purge</code>: reset to a clean environment</li>
</ul>
<p>Use the modules package to make Python available by running <code>module load python</code>.</p>
<hr/>
<h2 id="conda">Conda</h2>
<p>Once you have loaded the appropriate Python module, creating an isolated Python environment for each project using Conda or Python‚Äôs venvs is good practice. This ensures your dependencies are self-contained and avoids interference with other users or projects.</p>
<p>A conda installation is already available through the <code>python</code> module you just loaded. Before you use <code>conda</code> for the first time, run the following commands to correctly setup where the environment and packages will be stored:</p>
<pre><code class="language-bash">if [ ! -f ~/.bash_profile ]; then
  echo "if [ -f ~/.bashrc ]; then . ~/.bashrc; fi" &gt; ~/.bash_profile
fi
module add python
conda config --add pkgs_dirs $WORK/software/private/conda/pkgs
conda config --add envs_dirs $WORK/software/private/conda/envs
</code></pre>
<p>This has only to be done once.</p>
<hr/>
<p>You can now create a new environment with the following command:</p>
<pre><code class="language-bash">conda create --name &lt;ENV_NAME&gt; python=&lt;PYTHON_VERSION&gt;
</code></pre>
<p>Alternatively, you can of course also use an existing environment configuration as a basis (i.e., <code>conda env create -f environment.yml</code> or <code>conda create --name &lt;ENV_NAME&gt; --file requirements.txt</code>).
Once activated (<code>conda activate &lt;ENV_NAME&gt;</code>), you can install libraries using <code>pip</code> or <code>conda</code>. Install packages using an (interactive) job to ensure the hardware is correctly supported. As the compute nodes are not configured with internet access by default, it is important to configure the proxy so that additional software can be downloaded:</p>
<pre><code class="language-bash">export http_proxy=http://proxy:80
export https_proxy=http://proxy:80
</code></pre>
<p>For convenience, you may add these statements to your <code>.bashrc</code> file (which is called every time a new bash shell is initiated).</p>
<hr/>
<h1 id="scheduling-jobs">Scheduling Jobs</h1>
<p><strong>SLURM</strong> (Simple Linux Utility for Resource Management) is a workflow manager for scheduling jobs and managing cluster resources. It allows users to request specific resources (like GPUs, CPUs, or memory) and submit jobs to the cluster‚Äôs job queue.</p>
<hr/>
<h2 id="slurm-basics">SLURM Basics</h2>
<p>Before submitting jobs, it is crucial to understand the following SLURM commands:</p>
<ul>
<li><code>sinfo</code>: Provides <strong>information about the cluster‚Äôs nodes and their availability</strong>.
The output may look something like this and tells you which nodes are available and their current status (e.g., maintenance, reserved, allocated, idle, etc.):</li>
</ul>
<pre><code class="language-text">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
a40*         up 1-00:00:00      1  maint a1722
a40*         up 1-00:00:00      1   resv a1721
a40*         up 1-00:00:00      8    mix a[0121,0126,0128-0129,0225,0229,0329,0423]
a40*         up 1-00:00:00     26  alloc a[0122-0125,0127,0221-0224,0226-0228,0321-0328,0421-0422,1621-1624]
a40*         up 1-00:00:00      8   idle a[0424-0429,0521-0522]
</code></pre>
<ul>
<li><code>squeue</code>: Shows all of your <strong>scheduled jobs and their current status</strong> (queued/running).
<strong>ST:</strong> job status (R = Running, PD = Pending)
<strong>TIME:</strong> elapsed time since resource allocation
<strong>NODELIST(REASON):</strong> allocated node or reason for the job pending</li>
</ul>
<pre><code class="language-text">    JOBID PARTITION NAME                 USER     ST       TIME  TIME_LIMIT  NODES CPUS NODELIST(REASON)
    1234567       a40 run_experiment       &lt;USERID&gt; PD       0:00    23:59:00      1   16 (Priority)
    1234568       a40 run_experiment       &lt;USERID&gt;  R    2:39:34    23:59:00      1   16 a0603
</code></pre>
<ul>
<li><code>sbatch</code>: Used to <strong>submit batch jobs</strong> to the SLURM scheduler (e.g., <code>sbatch my_job_file.job</code>).
Once submitted, the job will be added to the queue and get assigned a unique job ID.</li>
<li><code>scancel</code>: <strong>Cancels a specific job</strong> given its job ID (e.g., <code>scancel 1234568</code>).</li>
</ul>
<hr/>
<h2 id="job-files">Job Files</h2>
<p>To run an experiment or script, you must create a job submission script specifying the resources required and the commands to execute. Here‚Äôs a simple job file that prints whether CUDA is available in PyTorch:</p>
<pre><code class="language-bash">#!/bin/bash -l
#SBATCH --job-name=run_experiment   # Name of Job
#SBATCH --output=results_%A.out     # File where outputs/errors will be saved
#SBATCH --time=00:59:00             # Time limit (hh:mm:ss)
#SBATCH --ntasks=1                  # Number of tasks
#SBATCH --gres=gpu:a40:1            # Request 1 GPU
#SBATCH --nodes=1                   # Request 1 node

module purge
module load python # load preinstalled python module (includes conda) 
conda activate &lt;ENV_NAME&gt; # activate environment

python -c "import torch; print('cuda available:', torch.cuda.is_available())"
</code></pre>
<p>You can specify higher resource requests like this:</p>
<pre><code class="language-bash"># Specifying the number of GPUs
#SBATCH --gres=gpu:a40:1 # Request 1 GPU
#SBATCH --gres=gpu:a40:4 # Request 4 GPUs

# Specifying the number of nodes
#SBATCH --nodes=1  # Request 1 node
#SBATCH --nodes=2  # Request 2 nodes
</code></pre>
<p>In the job file, specify the experiments/code you want to execute. Afterwards, you can submit the job via the <code>sbatch</code> command.</p>
<hr/>
<!-- _hide: true -->
<h2 id="job-arrays">Job Arrays</h2>
<p>Batch arrays enable you to run multiple similar jobs in one go. This can be useful when iterating over different model configurations (e.g., hyperparameter tuning) and is more convenient than manually managing and scheduling various jobs yourself.</p>
<p>With <em>job arrays</em>, SLURM enables you to run the same program with different parameters. The following example executes the <code>experiment.py</code> file with different learning rate parameters:</p>
<pre><code class="language-bash">#!/bin/bash -l
#SBATCH --job-name=run_experiment
#SBATCH --output=results_%A_%a.out
#SBATCH --time=00:59:00
#SBATCH --array=0-4
#SBATCH --ntasks=1
#SBATCH --gres=gpu:a40:1
#SBATCH --nodes=1

# Setup environment
module purge
module load python
conda activate &lt;ENV_NAME&gt;

# Define parameters for each task
LEARNING_RATES=("0.01" "0.001" "0.0001" "0.00001" "0.000001")
LR=${LEARNING_RATES[$SLURM_ARRAY_TASK_ID]}

# Run the program with the specified parameter
python experiment.py --lr=$LR
</code></pre>
<p>Here, <code>$SLURM_ARRAY_TASK_ID</code> is automatically set to the corresponding task ID. The total number of tasks is stored in <code>$SLURM_ARRAY_TASK_COUNT</code>. You can additionally specify the maximum number of concurrent jobs using the <code>%</code> operator (e.g., <code>--array=0-7%4</code> creates 8 tasks but only runs 4 of them at any time). The output is saved to a text file named <code>results_&lt;job_id&gt;_&lt;task_id&gt;</code> indicated by <code>%A</code> and <code>%a</code>, respectively. </p>
<p>For a more detailed introduction and advanced use cases see the <a href="https://slurm.schedmd.com/job_array.html">official SLURM documentation</a>. Another handy way of organizing and executing experiments with job arrays for hyperparameter sweeps is suggested by <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html#Job-Arrays">Phillip Lippe</a>. Instead of specifying the hyperparameters in the job file itself, it can be recomendable to use a configuration management framework like <a href="https://hydra.cc/docs/intro/">Hydra</a>.</p>
<hr/>
<h2 id="interactive-sessions">Interactive Sessions</h2>
<p>Interactive sessions allow you to access a compute node directly for debugging or experimentation. Unlike normal jobs, interactive sessions make it possible to continuously execute code without terminating the resource allocation after the initial script concludes. This can be useful for verifying the correctness of commands or debugging where waiting for resource allocation after every small change is unfeasible.</p>
<p>You can request an interactive session using the following command (same parameters as regular job file): </p>
<pre><code class="language-bash">salloc --gres=gpu:a40:1 --time=3:59:00
</code></pre>
<p>As soon as the resources are allocated, you are connected to the compute node and can directly interact with it via the command line (e.g., by running <code>module load python</code> when planning to use Python). </p>
<p>However, keep in mind two important points when using interactive sessions: </p>
<ol>
<li>When you disconnect from Alex (e.g., unstable internet connection, closing your laptop), the session will be canceled, and whatever you are currently running will be interrupted</li>
<li>The session does not automatically end once your script is finished running, and it may be idle for extended periods. Therefore, interactive sessions are less efficient and should only be used when necessary. </li>
</ol>
<hr/>
<h2 id="attaching-to-running-jobs">Attaching to Running Jobs</h2>
<p>If you want to monitor the resource utilization of your experiment, it is necessary to connect to the node allocated to your job. The following command allows you to attach to a running job from a login node:</p>
<pre><code class="language-bash">srun --jobid=&lt;JOB_ID&gt; --overlap --pty /bin/bash -l
</code></pre>
<p>After replacing <code>&lt;JOB_ID&gt;</code> with your specific job ID (can be obtained via <code>squeue</code>), you can use tools like <code>nvidia-smi</code> or <code>htop</code> to check how the available resources are utilized. </p>
<hr/>
<h1 id="best-practices_1">Best Practices</h1>
<hr/>
<h3 id="test-your-code">Test Your Code</h3>
<p>Make sure your code runs before scheduling large-scale experiments (e.g., test locally or with small subset of data). It is frustrating to have jobs fail mid-way, especially when the cluster is busy and allocation might take a while (also, we don‚Äôt want to waste our scarce GPU resources üò¢).</p>
<hr/>
<h3 id="monitor-your-resource-utilization">Monitor Your Resource Utilization</h3>
<p>Even when your code runs without errors and produces the expected results, it is still possible that it doesn‚Äôt make use of the hardware resources effectively. To diagnose such issues, you can use the <a href="https://monitoring.nhr.fau.de/">ClusterCockpit</a> that shows you the resource usage over the runtime of your jobs (e.g., CPU/GPU load and memory usage). That way, you can identify potnential bottleneck and implement your approach more efficiently. When launching for the first time, go to <em>User &gt; Your Accounts &gt; External Tools</em> in the HPC Portal.</p>
<hr/>
<h3 id="only-request-resources-you-need">Only Request Resources You Need</h3>
<p>Make sure to only request as many resources as you actually need. Besides not taking up GPUs that others could use, there‚Äôs also something in for you: the shorter the job and the fewer resources required, the faster resources will be allocated. That said, requesting too little (esp. runtime) may cause your experiments to be terminated too early or fail due to out-of-memory (OOM) errors. Knowing the requirements of your setup is important and factor in a little leeway. </p>
<hr/>
<h3 id="use-version-control">Use Version Control</h3>
<p>Despite some of the filesystem being backed up and creating regular snapshots, a risk of losing data remains. Therefore, be sure to regularly commit and push changes made to your codebase (e.g., via github).</p>
<hr/>
<h3 id="data-staging">Data Staging</h3>
<p>Staging your data can be helpful when working with models or data that require frequent IO operations (e.g., regularly loading many files). At the beginning of your job, copy/extract your data to the node-local and very fast <code>$TMPDIR</code> directory before executing your experiments. Compared to directly loading from the original directory (e.g., <code>$WORK</code>), this additional step can significantly speed up some workflows and improve resource utilization. </p>
<hr/>
<h1 id="troubleshooting-and-faqs">Troubleshooting and FAQs</h1>
<p>Let me know if you encounter issues that other people might have, and I can add them here. </p>
<hr/>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>This tutorial and the contents provided are partially built upon the following resources:</p>
<ul>
<li><a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html">‚ÄúWorking with the Snellius cluster‚Äù</a>, a guide by Phillip Lippe and Danilo de Goede</li>
<li><a href="https://doc.nhr.fau.de">Official Documentation</a> by the HPC people at FAU</li>
</ul>
<p>Thanks to <a href="https://lukasknobel.github.io">Lukas Knobel</a> for helpful comments and discussions.</p>
<hr/>
<div class="admonition note">
<p class="admonition-title">Now that we ended ‚Ä¶</p>
<p>Your feedback is important! If you encounter any issues while following the tutorial or have suggestions for improvements, please don‚Äôt hesitate to reach out. Additionally, if there are any, we‚Äôd love to hear your ideas on topics you‚Äôd like to see covered in future tutorials or documentation, we‚Äôd love to reach out. You can find my contact details <a href="https://jonaruthardt.github.io/">here</a>. Thank you for using this resource, and I hope it helped you get started using clusters for your own projects.</p>
</div>
<hr/>
<p>Slides available <a href="../slides/cluster_tutorial.html">here</a>
Practical resources available <a href="https://github.com/JonaRuthardt/HowToCluster">here</a></p>
<hr/></div>
</div>
</div>
<footer class="col-md-12">
<hr/>
<p>¬© Jona Ruthardt 2025</p>
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
</footer>
<script src="../js/bootstrap.bundle.min.js"></script>
<script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
<script src="../js/base.js"></script>
<script src="../search/main.js"></script>
<div aria-hidden="true" aria-labelledby="searchModalLabel" class="modal" id="mkdocs_search_modal" role="dialog" tabindex="-1">
<div class="modal-dialog modal-lg">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="searchModalLabel">Search</h4>
<button aria-label="Close" class="btn-close" data-bs-dismiss="modal" type="button"></button>
</div>
<div class="modal-body">
<p>From here you can search these documents. Enter your search terms below.</p>
<form>
<div class="form-group">
<input class="form-control" id="mkdocs-search-query" placeholder="Search..." title="Type search term here" type="search"/>
</div>
</form>
<div data-no-results-text="No results found" id="mkdocs-search-results"></div>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div><div aria-hidden="true" aria-labelledby="keyboardModalLabel" class="modal" id="mkdocs_keyboard_modal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
<button aria-label="Close" class="btn-close" data-bs-dismiss="modal" type="button"></button>
</div>
<div class="modal-body">
<table class="table">
<thead>
<tr>
<th style="width: 20%;">Keys</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td class="help shortcut"><kbd>?</kbd></td>
<td>Open this help</td>
</tr>
<tr>
<td class="next shortcut"><kbd>n</kbd></td>
<td>Next page</td>
</tr>
<tr>
<td class="prev shortcut"><kbd>p</kbd></td>
<td>Previous page</td>
</tr>
<tr>
<td class="search shortcut"><kbd>s</kbd></td>
<td>Search</td>
</tr>
</tbody>
</table>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>
